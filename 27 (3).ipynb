{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3PqfG8rZlzg"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aETbHMwSgzU4"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwRvAsStJiNv"
      },
      "source": [
        "**Text Summarizer with spAcy word vector sentence embeddings + KMeans Clustering with Visualization and Evaluation Metrics(ROUGE)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9WGY_S2ZrHl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "from rouge_score import rouge_scorer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQmACP2RgSFx"
      },
      "outputs": [],
      "source": [
        "#Preprocessing\n",
        "custom_stop_words = {\"the\", \"in\", \"on\", \"of\", \"and\", \"a\", \"an\"}\n",
        "\n",
        "def selective_lowercase(text):\n",
        "    tokens = text.split()\n",
        "    processed_tokens = []\n",
        "    for token in tokens:\n",
        "        token_core = re.sub(r'^\\W+|\\W+$', '', token)\n",
        "        if (token_core.isupper() and len(token_core) > 1) or re.search(r'\\d', token_core):\n",
        "            processed_tokens.append(token)\n",
        "        else:\n",
        "            processed_tokens.append(token.lower())\n",
        "    return \" \".join(processed_tokens)\n",
        "\n",
        "def selective_stopword_removal(text):\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in custom_stop_words]\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = selective_lowercase(sentence)\n",
        "    return selective_stopword_removal(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEZrQikBgmIf"
      },
      "outputs": [],
      "source": [
        "# --- Sample Sentences ---\n",
        "sentences = [\n",
        "    \"The Expanded Programme on Immunisation was launched by the WHO in 1974.\",\n",
        "    \"Smallpox was declared eradicated in 1980 after a successful global vaccination campaign.\",\n",
        "    \"The climate crisis is spurring disease outbreaks in vulnerable communities.\",\n",
        "    \"The UK is considering a significant cut to its support for global vaccine programs.\",\n",
        "    \"Polio remains endemic in just a few countries, but progress is steady.\"\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v0oZFvUgwwu"
      },
      "outputs": [],
      "source": [
        "# Preprocess sentences\n",
        "preprocessed_sentences = [preprocess_sentence(sent) for sent in sentences]\n",
        "\n",
        "# Embedding Setup\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def get_sentence_embedding(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    word_vectors = [token.vector for token in doc if token.has_vector and not token.is_stop]\n",
        "    return np.mean(word_vectors, axis=0) if word_vectors else doc.vector\n",
        "\n",
        "# Get sentence embeddings\n",
        "sentence_embeddings = np.array([get_sentence_embedding(sent) for sent in preprocessed_sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OioBWLJ5g7Fc"
      },
      "outputs": [],
      "source": [
        "# --- KMeans Summarization ---\n",
        "def kmeans_summarization(original_sentences, embeddings, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    kmeans.fit(embeddings)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "    cluster_labels = kmeans.labels_\n",
        "\n",
        "    selected_sentences = []\n",
        "    for i in range(n_clusters):\n",
        "        cluster_indices = [j for j, label in enumerate(cluster_labels) if label == i]\n",
        "        closest_idx = min(cluster_indices, key=lambda idx: np.linalg.norm(embeddings[idx] - cluster_centers[i]))\n",
        "        selected_sentences.append((closest_idx, original_sentences[closest_idx]))\n",
        "\n",
        "    selected_sentences.sort(key=lambda x: x[0])\n",
        "    return \" \".join(sent for _, sent in selected_sentences), cluster_labels\n",
        "\n",
        "# Generate summary and cluster labels\n",
        "generated_summary, cluster_labels = kmeans_summarization(sentences, sentence_embeddings, n_clusters=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxMCK8GkhF6Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- t-SNE Visualization ---\n",
        "def visualize_clusters(embeddings, cluster_labels, sentences):\n",
        "    n_samples = len(embeddings)\n",
        "    perplexity = min(5, n_samples - 1)  # Ensure perplexity is less than the number of samples\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(\n",
        "        reduced_embeddings[:, 0], reduced_embeddings[:, 1],\n",
        "        c=cluster_labels, cmap='viridis', s=150, edgecolors='black'  # Larger points, darker edges\n",
        "    )\n",
        "    plt.colorbar(scatter, label=\"Cluster ID\")\n",
        "\n",
        "    for i in range(len(embeddings)):\n",
        "        plt.annotate(\n",
        "            str(cluster_labels[i] + 1),  # Offset by +1\n",
        "            (reduced_embeddings[i, 0], reduced_embeddings[i, 1]),\n",
        "            fontsize=14, weight=\"bold\", color=\"black\", backgroundcolor=\"white\",\n",
        "            xytext=(5, 5), textcoords=\"offset points\"  # Better placement\n",
        "        )\n",
        "\n",
        "    plt.title(\"Sentence Clustering Visualization (t-SNE + KMeans)\")\n",
        "    plt.xlabel(\"t-SNE Dim 1\")\n",
        "    plt.ylabel(\"t-SNE Dim 2\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- Print cluster assignments ---\n",
        "    print(\"\\nCluster Assignments:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"Cluster {cluster_labels[i] + 1}: {sentence}\")\n",
        "\n",
        "# Show visualization\n",
        "visualize_clusters(sentence_embeddings, cluster_labels, sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBRpz325hI2h"
      },
      "outputs": [],
      "source": [
        "# --- ROUGE Comparison ---\n",
        "provided_summary = (\n",
        "    \"Global immunization initiatives have achieved notable milestones, including the WHO’s launch \"\n",
        "    \"of the Expanded Programme on Immunisation in 1974 and the eradication of smallpox in 1980. \"\n",
        "    \"However, emerging challenges—such as disease outbreaks fueled by the climate crisis and potential funding cuts, \"\n",
        "    \"as seen in the UK—highlight ongoing vulnerabilities. Meanwhile, steady progress continues against polio, even though \"\n",
        "    \"it remains endemic in a few countries.\"\n",
        ")\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(provided_summary, generated_summary)\n",
        "\n",
        "# --- Display Results ---\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)\n",
        "print(\"\\nROUGE Scores:\")\n",
        "for key, value in scores.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA5BJLhmJ-W0"
      },
      "source": [
        "**Multiple Document Summarization using a hybrid approach : LSA+TextRank(for abstractive summary generation)+Evaluation metrics (ROUGE)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUj9_T_P-i1P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQy44Em2-qHE"
      },
      "outputs": [],
      "source": [
        "src_1 = [\n",
        "    \"The Expanded Programme on Immunisation was launched by the WHO in 1974.\",\n",
        "    \"Smallpox was declared eradicated in 1980 after a successful global vaccination campaign.\",\n",
        "    \"The climate crisis is spurring disease outbreaks in vulnerable communities.\",\n",
        "    \"The UK is considering a significant cut to its support for global vaccine programs.\",\n",
        "    \"Polio remains endemic in just a few countries, but progress is steady.\"\n",
        "]\n",
        "\n",
        "src_2 = [\n",
        "    \"Vaccination efforts have drastically reduced child mortality rates worldwide.\",\n",
        "    \"WHO's immunization programs have saved millions of lives over the past decades.\",\n",
        "    \"Emerging diseases demand faster vaccine development and equitable distribution.\",\n",
        "    \"Public-private partnerships have played a crucial role in making vaccines accessible.\",\n",
        "    \"Investment in vaccine infrastructure is key to preventing future pandemics.\"\n",
        "]\n",
        "\n",
        "src_3 = [\n",
        "    \"Low-income countries rely heavily on international vaccine initiatives for immunization.\",\n",
        "    \"The cost of vaccine development has been offset by global funding strategies.\",\n",
        "    \"Gavi has supported immunization programs in over 70 countries, improving access.\",\n",
        "    \"Political instability affects vaccine distribution in conflict zones.\",\n",
        "    \"Researchers are working on next-generation vaccines to combat evolving viruses.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TntFEgC3-_WD"
      },
      "outputs": [],
      "source": [
        "# Manually written base summary\n",
        "base_summary = \"\"\"The WHO’s Expanded Programme on Immunisation, launched in 1974, has led to significant milestones,\n",
        "including the eradication of smallpox in 1980. However, new challenges arise as the climate crisis accelerates disease\n",
        "outbreaks and global vaccine funding faces potential cuts. Despite progress in reducing child mortality and combating\n",
        "diseases like polio, continued efforts are crucial. Public-private partnerships and global initiatives like Gavi play\n",
        "a key role in making vaccines accessible, especially in low-income nations. Meanwhile, political instability and evolving\n",
        "viruses threaten distribution, emphasizing the need for next-generation vaccines and stronger infrastructure investments\n",
        "to prevent future pandemics.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPj8LzqkDLBc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def lsa_summarization(sentences, num_sentences=3):\n",
        "    if len(sentences) == 0:\n",
        "        return \"No summary: Input text is empty.\"\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Ensure the number of components doesn't exceed available sentences\n",
        "    n_components = min(X.shape[0] - 1, X.shape[1], 1)  # SVD requires (n_samples > n_components)\n",
        "\n",
        "    if n_components <= 0:\n",
        "        return \"Not enough data for summarization.\"\n",
        "\n",
        "    svd = TruncatedSVD(n_components=n_components)\n",
        "    X_reduced = svd.fit_transform(X)\n",
        "\n",
        "    # Compute sentence scores using LSA\n",
        "    scores = np.argsort(-X_reduced[:, 0])  # Sort in descending order\n",
        "\n",
        "    # Prevent out-of-range errors\n",
        "    selected_sentences = min(num_sentences, len(sentences))\n",
        "    summary = [sentences[i] for i in scores[:selected_sentences]]\n",
        "\n",
        "    return \" \".join(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEdnG8nZEB0O"
      },
      "outputs": [],
      "source": [
        "lsa_summary_1 = lsa_summarization(src_1)\n",
        "lsa_summary_2 = lsa_summarization(src_2)\n",
        "lsa_summary_3 = lsa_summarization(src_3)\n",
        "\n",
        "print(\"LSA Summary for Source 1:\\n\", lsa_summary_1)\n",
        "print(\"\\nLSA Summary for Source 2:\\n\", lsa_summary_2)\n",
        "print(\"\\nLSA Summary for Source 3:\\n\", lsa_summary_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWFLnWA_FSkU"
      },
      "outputs": [],
      "source": [
        "!pip install summa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X91RziqaB5kA"
      },
      "outputs": [],
      "source": [
        "from summa import summarizer\n",
        "\n",
        "# Function to perform TextRank summarization\n",
        "def textrank_summarization(sentences, num_sentences=3):\n",
        "    text = \" \".join(sentences)  # Convert list of sentences to a paragraph\n",
        "    summary = summarizer.summarize(text, ratio=num_sentences/len(sentences))\n",
        "    return summary\n",
        "\n",
        "# Generate TextRank summaries\n",
        "textrank_summary_1 = textrank_summarization(src_1)\n",
        "textrank_summary_2 = textrank_summarization(src_2)\n",
        "textrank_summary_3 = textrank_summarization(src_3)\n",
        "\n",
        "# Print the summaries\n",
        "print(\"TextRank Summary for Source 1:\\n\", textrank_summary_1, \"\\n\")\n",
        "print(\"TextRank Summary for Source 2:\\n\", textrank_summary_2, \"\\n\")\n",
        "print(\"TextRank Summary for Source 3:\\n\", textrank_summary_3, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRC5uRRiFU5w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import networkx as nx\n",
        "\n",
        "# LSA Summarization (already defined)\n",
        "def lsa_summarization(sentences, num_sentences=3):\n",
        "    if len(sentences) == 0:\n",
        "        return \"No summary: Input text is empty.\"\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    n_components = min(X.shape[0] - 1, X.shape[1], 1)  # Ensure valid SVD components\n",
        "    if n_components <= 0:\n",
        "        return \"Not enough data for summarization.\"\n",
        "\n",
        "    svd = TruncatedSVD(n_components=n_components)\n",
        "    X_reduced = svd.fit_transform(X)\n",
        "\n",
        "    # Rank sentences using LSA\n",
        "    scores = np.argsort(-X_reduced[:, 0])\n",
        "\n",
        "    # Select top sentences\n",
        "    selected_sentences = min(num_sentences, len(sentences))\n",
        "    lsa_filtered_sentences = [sentences[i] for i in scores[:selected_sentences]]\n",
        "\n",
        "    return lsa_filtered_sentences  # Return list for next processing step\n",
        "\n",
        "# TextRank Summarization applied on LSA-filtered sentences\n",
        "def textrank_summarization(sentences, num_sentences=3):\n",
        "    if not sentences:\n",
        "        return \"No summary: No input sentences.\"\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    similarity_matrix = (X * X.T).toarray()\n",
        "\n",
        "\n",
        "    # Create a graph\n",
        "    graph = nx.from_numpy_array(similarity_matrix)\n",
        "\n",
        "    # Compute PageRank scores\n",
        "    scores = nx.pagerank(graph)\n",
        "\n",
        "    # Rank sentences by score\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    # Select top-ranked sentences\n",
        "    return \" \".join([s[1] for s in ranked_sentences[:num_sentences]])\n",
        "\n",
        "# Apply LSA summarization\n",
        "lsa_filtered_1 = lsa_summarization(src_1)\n",
        "lsa_filtered_2 = lsa_summarization(src_2)\n",
        "lsa_filtered_3 = lsa_summarization(src_3)\n",
        "\n",
        "# The output of lsa_summarization is now a list, so no need to split\n",
        "lsa_sentences_1 = lsa_filtered_1\n",
        "lsa_sentences_2 = lsa_filtered_2\n",
        "lsa_sentences_3 = lsa_filtered_3\n",
        "\n",
        "# Apply TextRank summarization\n",
        "hybrid_summary_1 = textrank_summarization(lsa_sentences_1)\n",
        "hybrid_summary_2 = textrank_summarization(lsa_sentences_2)\n",
        "hybrid_summary_3 = textrank_summarization(lsa_sentences_3)\n",
        "\n",
        "# Print final hybrid summaries\n",
        "print(\"Hybrid Summary for Source 1:\\n\", hybrid_summary_1)\n",
        "print(\"\\nHybrid Summary for Source 2:\\n\", hybrid_summary_2)\n",
        "print(\"\\nHybrid Summary for Source 3:\\n\", hybrid_summary_3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q271GA7BI3pC"
      },
      "outputs": [],
      "source": [
        "!pip install rouge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpcRK4UPGBRU"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def evaluate_rouge(reference_summary, generated_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
        "    return scores\n",
        "\n",
        "# Manually Written Base Summary (Reference)\n",
        "base_summary = \"\"\"The WHO’s Expanded Programme on Immunisation, launched in 1974, has led to significant milestones,\n",
        "including the eradication of smallpox in 1980. However, new challenges arise as the climate crisis accelerates disease\n",
        "outbreaks and global vaccine funding faces potential cuts. Despite progress in reducing child mortality and combating\n",
        "diseases like polio, continued efforts are crucial. Public-private partnerships and global initiatives like Gavi play\n",
        "a key role in making vaccines accessible, especially in low-income nations. Meanwhile, political instability and evolving\n",
        "viruses threaten distribution, emphasizing the need for next-generation vaccines and stronger infrastructure investments\n",
        "to prevent future pandemics.\"\"\"\n",
        "\n",
        "# Hybrid Summaries (Generated)\n",
        "hybrid_summary = \"\"\"The climate crisis is spurring disease outbreaks in vulnerable communities. The Expanded Programme on\n",
        "Immunisation was launched by the WHO in 1974. The UK is considering a significant cut to its support for global vaccine\n",
        "programs. Public-private partnerships have played a crucial role in making vaccines accessible. Vaccination efforts have\n",
        "drastically reduced child mortality rates worldwide. Investment in vaccine infrastructure is key to preventing future pandemics.\n",
        "Low-income countries rely heavily on international vaccine initiatives for immunization. Gavi has supported immunization programs\n",
        "in over 70 countries, improving access. Political instability affects vaccine distribution in conflict zones.\"\"\"\n",
        "\n",
        "# Compute ROUGE Scores\n",
        "rouge_scores = evaluate_rouge(base_summary, hybrid_summary)\n",
        "\n",
        "# Print Results\n",
        "print(\"ROUGE Scores:\\n\", rouge_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55J13GJVeIPi"
      },
      "source": [
        "**Multiple Types Of Documents(here we have used the articles and image captions found in the article) Text Summarization using LSA+TextRank Approach**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hhNoe9fZmjY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmyQYV6FZ0JL"
      },
      "outputs": [],
      "source": [
        "# Input: Article + Image Captions\n",
        "documents = [\n",
        "    \"Low-income countries rely heavily on international vaccine initiatives for immunization.\",\n",
        "    \"The cost of vaccine development has been offset by global funding strategies.\",\n",
        "    \"Gavi has supported immunization programs in over 70 countries, improving access.\",\n",
        "    \"Political instability affects vaccine distribution in conflict zones.\",\n",
        "    \"Researchers are working on next-generation vaccines to combat evolving viruses.\",\n",
        "\n",
        "    # Image Captions\n",
        "    \"A health worker administers a vaccine to a child in a rural clinic, illustrating Gavi's efforts to improve immunization access in low-income countries.\",\n",
        "    \"A mother holds her child while receiving a vaccine in a conflict-affected region, highlighting the challenges of vaccine distribution in such areas.\",\n",
        "    \"Researchers in a laboratory developing next-generation vaccines to combat evolving viruses.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv_v5B1HZ45e"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentences = nltk.sent_tokenize(\" \".join(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5QkKE3QatPe"
      },
      "outputs": [],
      "source": [
        "# Function to perform LSA + TextRank Summarization\n",
        "def lsa_textrank_summarization(sentences, num_sentences=3):\n",
        "    if not sentences:\n",
        "        return \"No summary: Input text is empty.\"\n",
        "\n",
        "    num_sentences = min(num_sentences, len(sentences))  # Prevent IndexError\n",
        "\n",
        "    # Convert sentences into TF-IDF features\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Apply LSA with a higher number of components\n",
        "    svd = TruncatedSVD(n_components=min(2, X.shape[1]), n_iter=100)\n",
        "    svd.fit(X)\n",
        "    # Rank sentences based on importance\n",
        "    importance = np.argsort(svd.components_[0])\n",
        "    top_sentence_indices = importance[:-(num_sentences+1):-1]\n",
        "\n",
        "    # Ensure selected sentences do not exceed available range\n",
        "    selected_sentences = [sentences[i] for i in top_sentence_indices if i < len(sentences)]\n",
        "\n",
        "    # If not enough sentences, take additional ones based on TextRank\n",
        "    if len(selected_sentences) < num_sentences:\n",
        "        graph = nx.Graph()\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            graph.add_node(i, text=sentence)\n",
        "        similarity_matrix = X * X.T\n",
        "        nx.set_edge_attributes(graph, {(i, j): {'weight': similarity_matrix[i, j]} for i in range(len(sentences)) for j in range(i+1, len(sentences))})\n",
        "        scores = nx.pagerank(graph)\n",
        "        additional_sentences = sorted(scores, key=scores.get, reverse=True)[:num_sentences - len(selected_sentences)]\n",
        "        selected_sentences.extend([sentences[i] for i in additional_sentences if i not in top_sentence_indices])\n",
        "\n",
        "    return \" \".join(selected_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMtknlOXa8YH"
      },
      "outputs": [],
      "source": [
        "# Generate summary\n",
        "summary = lsa_textrank_summarization(sentences, num_sentences=4)\n",
        "\n",
        "# Print final summary\n",
        "print(\"\\nFinal Multi-Document Summary:\\n\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hilAp7oPa_C_"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "# Human-written reference summary (manually crafted)\n",
        "reference_summary = \"\"\"Low-income countries rely on international vaccine initiatives for immunization.\n",
        "Gavi has played a major role in improving vaccine accessibility in over 70 countries.\n",
        "Political instability challenges vaccine distribution in conflict zones.\n",
        "Researchers are developing next-generation vaccines to combat evolving viruses.\"\"\"\n",
        "\n",
        "# Compute ROUGE Scores\n",
        "def evaluate_summary(reference_summary, generated_summary):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
        "    return scores\n",
        "\n",
        "# Evaluate the LSA + TextRank summary\n",
        "rouge_scores = evaluate_summary(reference_summary, summary)\n",
        "\n",
        "# Print Results\n",
        "print(\"\\nROUGE Scores:\")\n",
        "for metric, score in rouge_scores[0].items():\n",
        "    print(f\"{metric}: Precision={score['p']:.4f}, Recall={score['r']:.4f}, F1-score={score['f']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTv6hr3hEY3x"
      },
      "source": [
        "**Fine Tuning word and sentence embeddings models**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo7Mp9X6_wHN"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQQrUhJL_wmw"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsZB0IwNAgm_"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVW2XlZWAv2f"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anuq5PI-BAsg"
      },
      "outputs": [],
      "source": [
        "sample_article = '''\n",
        "Parsing is the process of analyzing a sentence's grammatical structure to determine relationships between words. In NLP, parsing can be classified into syntactic parsing, which focuses on sentence structure, and semantic parsing, which extracts meaning. Constituency parsing breaks a sentence into hierarchical phrases, while dependency parsing maps direct word dependencies. Parsing is crucial in machine translation, question answering, and information extraction to ensure accurate text processing. One major challenge in parsing is handling ambiguous sentence structures, which require advanced models for disambiguation. Stemming is a process in NLP that removes prefixes and suffixes to reduce words to their root form. Common stemming algorithms, such as the Porter Stemmer and Snowball Stemmer, help standardize words like \"running\", \"runner\", and \"runs\" into \"run\". Stemming is widely used in search engines, text mining, and information retrieval to match word variations efficiently. A limitation of stemming is that it may produce incorrect root words, such as stemming \"happiness\" to \"happi\", leading to reduced accuracy. Despite its limitations, stemming is preferred in high-speed NLP applications where precision is less critical than performance. Lemmatization is a text normalization technique that reduces words to their base dictionary form while maintaining meaning. Unlike stemming, which may produce invalid words, lemmatization ensures that words like \"better\" are correctly mapped to \"good\". Lemmatization uses morphological analysis and part-of-speech tagging to determine the correct lemma for each word. This technique improves accuracy in search engines, text summarization, and document classification. Since lemmatization relies on linguistic rules, it is computationally more expensive than stemming but yields better results in context-aware NLP applications. Tokenization is the process of splitting text into smaller units called tokens, which can be words, subwords, or characters. Word-level tokenization treats each word separately, while subword tokenization (e.g., Byte Pair Encoding) helps handle unknown words effectively. Tokenization is an essential preprocessing step in language modeling, machine translation, and speech recognition. Challenges in tokenization include handling punctuation, contractions, and compound words, which can affect downstream NLP tasks. Advanced tokenization techniques, such as WordPiece and SentencePiece, improve performance in modern transformer-based models. Part-of-Speech (POS) tagging is the process of assigning grammatical categories like nouns, verbs, adjectives, and adverbs to words in a sentence. POS tagging helps in syntactic parsing, named entity recognition (NER), and sentiment analysis by identifying word roles in a sentence. Machine learning-based POS taggers outperform rule-based methods by leveraging context and large annotated datasets. Ambiguity is a challenge in POS tagging, as words like \"lead\" can be either a verb or a noun depending on the context. Modern POS tagging models, such as those using recurrent neural networks (RNNs) and transformers, achieve high accuracy across different languages.\n",
        "'''\n",
        "\n",
        "reference_summary = '''\n",
        "Parsing in NLP involves analyzing grammatical structure to identify relationships between words, playing a vital role in tasks like translation and information extraction. Stemming reduces words to their root forms using algorithms like Porter and Snowball, improving performance in tasks such as search and text mining despite potential accuracy trade-offs. Lemmatization, unlike stemming, maps words to their dictionary forms based on linguistic rules, offering better context-aware normalization for applications like summarization and classification. Tokenization splits text into words or subwords, enabling preprocessing for models while handling challenges like punctuation and compound words. Part-of-Speech tagging assigns grammatical roles to words using advanced models like RNNs and transformers, aiding in tasks such as parsing and sentiment analysis.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lfYr7_MBPPw"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "sentences = sent_tokenize(sample_article)\n",
        "\n",
        "#  Sentence Embedding using SBERT\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentence_embeddings = sbert_model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "#  Document embedding and cosine similarity for relevance\n",
        "doc_embedding = sentence_embeddings.mean(dim=0)\n",
        "cosine_scores = util.pytorch_cos_sim(doc_embedding, sentence_embeddings)[0]\n",
        "\n",
        "#  Select top-k relevant sentences\n",
        "top_k = 7\n",
        "top_indices = np.argsort(-cosine_scores.cpu())[:top_k]\n",
        "selected_sentences = [sentences[idx] for idx in sorted(top_indices)]\n",
        "selected_text = \" \".join(selected_sentences)\n",
        "\n",
        "print(\" Selected Relevant Sentences:\\n\")\n",
        "print(selected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tNUSXX37BbNu"
      },
      "outputs": [],
      "source": [
        "#  Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "#  Tokenize input\n",
        "inputs = tokenizer(selected_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "\n",
        "#  Generate summary\n",
        "summary_ids = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    num_beams=4,\n",
        "    max_length=\n",
        "    150,\n",
        "    min_length=60,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "#  Decode output\n",
        "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n Generated Abstractive Summary:\\n\")\n",
        "print(generated_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "seql4jQpCeJx"
      },
      "outputs": [],
      "source": [
        "#  ROUGE evaluation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "print(\"\\n ROUGE Scores:\")\n",
        "for key, value in scores.items():\n",
        "    print(f\"{key}: Precision={value.precision:.2f}, Recall={value.recall:.2f}, F1={value.fmeasure:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tb6geY2TCvlm"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "\n",
        "# Load pretrained BART\n",
        "model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Concatenate the selected relevant sentences\n",
        "input_text = \" \".join(selected_sentences)\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer([input_text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "#  Beam Search + High Temperature Generation\n",
        "summary_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    num_beams=5,              # beam search width\n",
        "    length_penalty=1.0,\n",
        "    max_length=150,\n",
        "    min_length=40,\n",
        "    early_stopping=True,\n",
        "    do_sample=True,          # sampling ON\n",
        "    top_k=50,                # Top-k sampling\n",
        "    temperature=1.6,         #  high temperature = more randomness\n",
        ")\n",
        "\n",
        "# Decode summary\n",
        "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\" Generated Abstractive Summary:\\n\", generated_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "83lWU1C6Db5E"
      },
      "outputs": [],
      "source": [
        "#  ROUGE evaluation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "print(\"\\n ROUGE Scores:\")\n",
        "for key, value in scores.items():\n",
        "    print(f\"{key}: Precision={value.precision:.2f}, Recall={value.recall:.2f}, F1={value.fmeasure:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HJOg4qiFfU3"
      },
      "source": [
        "**FINE TUNING WITH CUSTOM DATA**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehKTqTtyZQ31"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets rouge_score --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzo3a1fkdiH5"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "from rouge_score import rouge_scorer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjDvlM75dkUb"
      },
      "outputs": [],
      "source": [
        "# Sample Pair 1\n",
        "sample_article_1 = \"\"\"\n",
        "Parsing is the process of analyzing a sentence's grammatical structure to determine relationships between words. In NLP, parsing can be classified into syntactic parsing, which focuses on sentence structure, and semantic parsing, which extracts meaning. Constituency parsing breaks a sentence into hierarchical phrases, while dependency parsing maps direct word dependencies. Parsing is crucial in machine translation, question answering, and information extraction to ensure accurate text processing. One major challenge in parsing is handling ambiguous sentence structures, which require advanced models for disambiguation. Stemming is a process in NLP that removes prefixes and suffixes to reduce words to their root form. Common stemming algorithms, such as the Porter Stemmer and Snowball Stemmer, help standardize words like \"running\", \"runner\", and \"runs\" into \"run\". Stemming is widely used in search engines, text mining, and information retrieval to match word variations efficiently. A limitation of stemming is that it may produce incorrect root words, such as stemming \"happiness\" to \"happi\", leading to reduced accuracy. Despite its limitations, stemming is preferred in high-speed NLP applications where precision is less critical than performance. Lemmatization is a text normalization technique that reduces words to their base dictionary form while maintaining meaning. Unlike stemming, which may produce invalid words, lemmatization ensures that words like \"better\" are correctly mapped to \"good\". Lemmatization uses morphological analysis and part-of-speech tagging to determine the correct lemma for each word. This technique improves accuracy in search engines, text summarization, and document classification. Since lemmatization relies on linguistic rules, it is computationally more expensive than stemming but yields better results in context-aware NLP applications. Tokenization is the process of splitting text into smaller units called tokens, which can be words, subwords, or characters. Word-level tokenization treats each word separately, while subword tokenization (e.g., Byte Pair Encoding) helps handle unknown words effectively. Tokenization is an essential preprocessing step in language modeling, machine translation, and speech recognition. Challenges in tokenization include handling punctuation, contractions, and compound words, which can affect downstream NLP tasks. Advanced tokenization techniques, such as WordPiece and SentencePiece, improve performance in modern transformer-based models. Part-of-Speech (POS) tagging is the process of assigning grammatical categories like nouns, verbs, adjectives, and adverbs to words in a sentence. POS tagging helps in syntactic parsing, named entity recognition (NER), and sentiment analysis by identifying word roles in a sentence. Machine learning-based POS taggers outperform rule-based methods by leveraging context and large annotated datasets. Ambiguity is a challenge in POS tagging, as words like \"lead\" can be either a verb or a noun depending on the context. Modern POS tagging models, such as those using recurrent neural networks (RNNs) and transformers, achieve high accuracy across different languages.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary_1 = \"\"\"\n",
        "Parsing in NLP involves analyzing grammatical structure to identify relationships between words, playing a vital role in tasks like machine translation, question answering, and information extraction. Syntactic and semantic parsing break down sentence structure and meaning, while stemming and lemmatization normalize word forms for efficient search and text mining. Tokenization splits text into manageable units, and POS tagging assigns grammatical roles to support tasks like NER and sentiment analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Sample Pair 2\n",
        "sample_article_2 = \"\"\"\n",
        "In Natural Language Processing (NLP), parsing is a fundamental technique that dissects a sentence’s grammatical structure to reveal the relationships among words. It can be divided into syntactic parsing, which primarily addresses sentence structure, and semantic parsing, which focuses on extracting meaning. Constituency parsing organizes a sentence into nested phrases, whereas dependency parsing connects words via direct dependencies. Accurate parsing is essential for applications such as machine translation, question answering, and information extraction, although ambiguous structures require advanced disambiguation models. Stemming, which removes affixes to derive word roots, is implemented through algorithms like the Porter Stemmer and Snowball Stemmer and is vital in search and text mining. However, stemming can sometimes yield imprecise roots, for instance reducing \"happiness\" to \"happi\". To counteract this, lemmatization converts words to their base dictionary forms more accurately, though at a higher computational cost. Tokenization divides text into tokens—be they words, subwords, or characters—and is a crucial preprocessing step in language modeling and translation. Effective tokenization must manage punctuation, contractions, and compound words, with modern techniques such as WordPiece and SentencePiece addressing these challenges. Additionally, Part-of-Speech tagging assigns grammatical categories to words, thereby clarifying their roles in a sentence and enhancing tasks like sentiment analysis and NER through advanced models based on RNNs and transformers.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary_2 = \"\"\"\n",
        "NLP parsing involves dissecting sentence structure to reveal word relationships, a critical component for translation, question answering, and information extraction. This process, which includes both syntactic and semantic parsing, is complemented by stemming and lemmatization to normalize word forms. Tokenization and POS tagging further prepare text for advanced NLP tasks by breaking it into tokens and assigning grammatical roles.\n",
        "\"\"\"\n",
        "\n",
        "# Sample Pair 3\n",
        "sample_article_3 = \"\"\"\n",
        "Parsing is a key process in NLP that involves systematically analyzing a sentence’s grammatical structure to determine the relationships between words. It encompasses both syntactic parsing, which examines the structural organization of sentences, and semantic parsing, which aims to extract the underlying meaning. Techniques such as constituency parsing divide a sentence into hierarchical components, while dependency parsing identifies direct relationships between words. This process is vital for applications including machine translation, question answering, and information extraction, though ambiguous sentence constructions remain challenging. In addition to parsing, NLP utilizes stemming to remove prefixes and suffixes and reduce words to their roots using algorithms like the Porter and Snowball stemmers. However, stemming may sometimes yield imprecise outputs, prompting the use of lemmatization, which converts words to their canonical forms using morphological analysis and part-of-speech tagging. Tokenization, which splits text into tokens (words, subwords, or characters), is another essential preprocessing step that supports tasks such as language modeling and speech recognition. Modern tokenization methods like Byte Pair Encoding, WordPiece, and SentencePiece are designed to handle challenges like punctuation and compound words. Finally, Part-of-Speech tagging assigns grammatical categories to words, thereby aiding in the extraction of key information for downstream NLP applications.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary_3 = \"\"\"\n",
        "Parsing in NLP involves systematically analyzing sentence structure to uncover word relationships, a process critical for translation, question answering, and information extraction. Syntactic and semantic parsing work in tandem, while stemming and lemmatization normalize word forms. Tokenization breaks text into tokens, and POS tagging assigns grammatical roles, together enabling effective language processing.\n",
        "\"\"\"\n",
        "\n",
        "# Sample Pair 4\n",
        "sample_article_4 = \"\"\"\n",
        "In NLP, parsing plays a crucial role in understanding language by analyzing sentence structures. There are two main forms: syntactic parsing, which focuses on the grammatical structure, and semantic parsing, which seeks to extract meaning. Hierarchical techniques such as constituency parsing and dependency parsing help structure sentences for applications like machine translation and question answering. However, ambiguity in sentence construction poses significant challenges that require sophisticated disambiguation models. In addition, text normalization processes like stemming and lemmatization are employed to reduce word forms to their roots or canonical forms. While stemming is fast and useful for high-speed applications, it can sometimes produce errors; lemmatization, though more computationally intensive, offers higher accuracy. Moreover, tokenization—which splits text into words or subwords—is critical for modern language models, and Part-of-Speech tagging assigns roles to words, further aiding tasks such as sentiment analysis and information retrieval.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary_4 = \"\"\"\n",
        "Parsing in NLP involves analyzing sentence structure through both syntactic and semantic methods to extract meaning. Techniques like constituency and dependency parsing support translation and question answering, while normalization via stemming and lemmatization ensures accurate word forms. Tokenization and POS tagging further prepare text for advanced NLP applications.\n",
        "\"\"\"\n",
        "\n",
        "# Sample Pair 5\n",
        "sample_article_5 = \"\"\"\n",
        "In the realm of Natural Language Processing, parsing is essential for understanding text. It involves breaking down sentences into grammatical components to reveal relationships between words. Syntactic parsing examines sentence structure, whereas semantic parsing focuses on the underlying meaning. This process is bolstered by techniques such as constituency parsing and dependency parsing, which organize words into phrases and establish direct relationships, respectively. To enhance processing, NLP systems employ stemming to remove inflectional endings and lemmatization to convert words to their dictionary form. Moreover, tokenization divides text into tokens, which is vital for tasks like speech recognition and machine translation, while POS tagging assigns each word a grammatical label, aiding further analysis such as sentiment evaluation and named entity recognition.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary_5 = \"\"\"\n",
        "Parsing in NLP is fundamental for analyzing grammatical structures and extracting meaning. It leverages both syntactic and semantic parsing, alongside normalization methods like stemming and lemmatization, as well as tokenization and POS tagging, to prepare text for advanced processing.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knO1s-v4dniB"
      },
      "outputs": [],
      "source": [
        "data_dict = {\n",
        "    \"article\": [\n",
        "        sample_article_1,\n",
        "        sample_article_2,\n",
        "        sample_article_3,\n",
        "        sample_article_4,\n",
        "        sample_article_5\n",
        "    ],\n",
        "    \"summary\": [\n",
        "        reference_summary_1,\n",
        "        reference_summary_2,\n",
        "        reference_summary_3,\n",
        "        reference_summary_4,\n",
        "        reference_summary_5\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict(data_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGv94ciKdpin"
      },
      "outputs": [],
      "source": [
        "def preprocess(batch):\n",
        "    model_inputs = tokenizer(batch[\"article\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(batch[\"summary\"], max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOVzI8dIdr-m"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bart-finetuned-custom\",\n",
        "    num_train_epochs=10,  # Increase epochs for better learning\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"no\",   # For demonstration, we disable evaluation here\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYQDV2h_dvfw"
      },
      "outputs": [],
      "source": [
        "def post_process(summary_text):\n",
        "    # Remove duplicate sentences using NLTK's sentence tokenizer.\n",
        "    import nltk\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    sentences = sent_tokenize(summary_text)\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sent in sentences:\n",
        "        if sent not in seen:\n",
        "            unique_sentences.append(sent)\n",
        "            seen.add(sent)\n",
        "    return \" \".join(unique_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Neu0XYPGd0UM"
      },
      "outputs": [],
      "source": [
        "def generate_summary(article_text, use_sampling=False):\n",
        "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    if use_sampling:\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            do_sample=True,\n",
        "            temperature=1.0,           # High temperature for more diversity\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            max_length=256,\n",
        "            length_penalty=1.6,\n",
        "            early_stopping=True\n",
        "        )\n",
        "    else:\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=256,\n",
        "            num_beams=3,\n",
        "            no_repeat_ngram_size=3,\n",
        "            length_penalty=1.6,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    raw_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return post_process(raw_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA33gO8id3dj"
      },
      "outputs": [],
      "source": [
        "generated_summary = generate_summary(sample_article_1, use_sampling=True)\n",
        "print(\"Generated Summary:\\n\", generated_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDoxAcrtd8N_"
      },
      "outputs": [],
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference_summary_1, generated_summary)\n",
        "\n",
        "print(\"\\nROUGE Scores:\")\n",
        "for key in rouge_scores:\n",
        "    score = rouge_scores[key]\n",
        "    print(f\"{key}: Precision={score.precision:.2f}, Recall={score.recall:.2f}, F1={score.fmeasure:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFLsMsu9fTjy"
      },
      "outputs": [],
      "source": [
        "from bert_score import score as bert_score\n",
        "P, R, F1 = bert_score([generated_summary], [reference_summary_1], lang=\"en\", verbose=True)\n",
        "print(\"\\nBERTScore:\")\n",
        "print(f\"Precision: {P.mean().item():.4f}, Recall: {R.mean().item():.4f}, F1: {F1.mean().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0IL0gf1fXFC"
      },
      "outputs": [],
      "source": [
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlQQzvsHfcvQ"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88QLuhklh4v_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets rouge_score matplotlib nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oawlirrb8HOq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import sent_token\n",
        "ize\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download(\"punkt\", quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gptQVWm98aAI"
      },
      "outputs": [],
      "source": [
        "sample_article = \"\"\"\n",
        "Parsing is the process of analyzing a sentence's grammatical structure to determine relationships between words. In NLP, parsing can be classified into syntactic parsing, which focuses on sentence structure, and semantic parsing, which extracts meaning. Constituency parsing breaks a sentence into hierarchical phrases, while dependency parsing maps direct word dependencies. Parsing is crucial in machine translation, question answering, and information extraction to ensure accurate text processing. One major challenge in parsing is handling ambiguous sentence structures, which require advanced models for disambiguation. Stemming is a process in NLP that removes prefixes and suffixes to reduce words to their root form. Common stemming algorithms, such as the Porter Stemmer and Snowball Stemmer, help standardize words like \"running\", \"runner\", and \"runs\" into \"run\". Stemming is widely used in search engines, text mining, and information retrieval to match word variations efficiently. A limitation of stemming is that it may produce incorrect root words, such as stemming \"happiness\" to \"happi\", leading to reduced accuracy. Despite its limitations, stemming is preferred in high-speed NLP applications where precision is less critical than performance. Lemmatization is a text normalization technique that reduces words to their base dictionary form while maintaining meaning. Unlike stemming, which may produce invalid words, lemmatization ensures that words like \"better\" are correctly mapped to \"good\". Lemmatization uses morphological analysis and part-of-speech tagging to determine the correct lemma for each word. This technique improves accuracy in search engines, text summarization, and document classification. Since lemmatization relies on linguistic rules, it is computationally more expensive than stemming but yields better results in context-aware NLP applications. Tokenization is the process of splitting text into smaller units called tokens, which can be words, subwords, or characters. Word-level tokenization treats each word separately, while subword tokenization (e.g., Byte Pair Encoding) helps handle unknown words effectively. Tokenization is an essential preprocessing step in language modeling, machine translation, and speech recognition. Challenges in tokenization include handling punctuation, contractions, and compound words, which can affect downstream NLP tasks. Advanced tokenization techniques, such as WordPiece and SentencePiece, improve performance in modern transformer-based models. Part-of-Speech (POS) tagging is the process of assigning grammatical categories like nouns, verbs, adjectives, and adverbs to words in a sentence. POS tagging helps in syntactic parsing, named entity recognition (NER), and sentiment analysis by identifying word roles in a sentence. Machine learning-based POS taggers outperform rule-based methods by leveraging context and large annotated datasets. Ambiguity is a challenge in POS tagging, as words like \"lead\" can be either a verb or a noun depending on the context. Modern POS tagging models, such as those using recurrent neural networks (RNNs) and transformers, achieve high accuracy across different languages.\n",
        "\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"\n",
        "Parsing in NLP involves analyzing grammatical structure to identify relationships between words, playing a vital role in tasks like translation and information extraction. Stemming reduces words to their root forms using algorithms like Porter and Snowball, improving performance in tasks such as search and text mining despite potential accuracy trade-offs. Lemmatization, unlike stemming, maps words to their dictionary forms based on linguistic rules, offering better context-aware normalization for applications like summarization and classification. Tokenization splits text into words or subwords, enabling preprocessing for models while handling challenges like punctuation and compound words. Part-of-Speech tagging assigns grammatical roles to words using advanced models like RNNs and transformers, aiding in tasks such as parsing and sentiment analysis.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpyWg3Cv8cji"
      },
      "outputs": [],
      "source": [
        "model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to output attentions and hidden states\n",
        "model.config.output_attentions = True\n",
        "model.config.output_hidden_states = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REDqdwj18juI"
      },
      "outputs": [],
      "source": [
        "# Encoder Part\n",
        "# Tokenize the sample article\n",
        "inputs = tokenizer(sample_article, return_tensors='pt', max_length=1024, truncation=True)\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6sHPrrw8qom"
      },
      "outputs": [],
      "source": [
        "\n",
        "encoder_outputs = model.model.encoder(input_ids, attention_mask=attention_mask)\n",
        "# encoder_outputs.attentions is a tuple: one element per encoder layer.\n",
        "# For visualization, pick the attention matrix from the first encoder layer, head 0.\n",
        "encoder_attentions = encoder_outputs.attentions[0]  # shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "enc_attn_matrix = encoder_attentions[0, 0].detach().cpu().numpy()  # For first sample, first head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMcwOOG-8w-9"
      },
      "outputs": [],
      "source": [
        "# Print sample tokenized input and positional embedding samples:\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print(\"Tokenized Input:\")\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzkWyjJn81W1"
      },
      "outputs": [],
      "source": [
        "pos_embed = model.model.encoder.embed_positions.weight.detach().cpu().numpy()\n",
        "print(\"\\nShape of Positional Embeddings:\", pos_embed.shape)\n",
        "print(\"First 10 positional embeddings (first 5 dimensions):\")\n",
        "for i in range(10):\n",
        "    print(f\"Position {i}: {pos_embed[i][:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Js6skES59tED"
      },
      "outputs": [],
      "source": [
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qCN7_-0R-AKv"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model returns attention and hidden state outputs\n",
        "model.config.output_attentions = True\n",
        "model.config.output_hidden_states = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wmkfmkT_FZO"
      },
      "outputs": [],
      "source": [
        "encoder_input_ids = tokenizer(\"Parsing is the process of analyzing a sentence's grammatical structure to determine relationships between words. In NLP, parsing can be classified into syntactic parsing, which focuses on sentence structure, and semantic parsing, which extracts meaning. Constituency parsing breaks a sentence into hierarchical phrases, while dependency parsing maps direct word dependencies..\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    decoder_input_ids = tokenizer(\"Parsing in NLP involves analyzing grammatical structure to identify relationships between words, playing a vital role in tasks like translation and information extraction..\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
        "\n",
        "outputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n",
        "\n",
        "encoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n",
        "decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU5UrJhs_mE0"
      },
      "outputs": [],
      "source": [
        "from bertviz import model_view\n",
        "model_view(\n",
        "    encoder_attention=outputs.encoder_attentions,\n",
        "    decoder_attention=outputs.decoder_attentions,\n",
        "    cross_attention=outputs.cross_attentions,\n",
        "    encoder_tokens= encoder_text,\n",
        "    decoder_tokens = decoder_text\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RU7RgHy_oOx"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<script>require.config({paths: {d3: 'https://d3js.org/d3.v5.min'}});</script>\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Meb16xbdWWAS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}